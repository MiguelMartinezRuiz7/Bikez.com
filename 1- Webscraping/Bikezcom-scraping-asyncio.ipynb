{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17339a0-fb1e-4000-9d69-24e6d97e91d5",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "from requests_html import AsyncHTMLSession\n",
    "import time\n",
    "import os\n",
    "\n",
    "scraped_FILE = \"scraped_urls.txt\"\n",
    "CSV_FILE = \"Bikez-All-Years.csv\"\n",
    "\n",
    "# Column order:\n",
    "column_order = [\n",
    "    # General information\n",
    "    \"Motorcycle name\", \"Model year\", \"Category\", \"Price as new\",\n",
    "    # Engine and transmission\n",
    "    \"Engine size\", \"Type of engine\", \"Power output\", \"Torque\", \"Transmission type\", \"Clutch\", \"Fuel consumption\",\n",
    "    # Chassis, suspension, brakes and wheels\n",
    "    \"Front tire\", \"Rear tire\", \"Front brakes\", \"Rear brakes\",\n",
    "    # Physical measures and capacities\n",
    "    \"Weight incl. oil, gas, etc\", \"Dry weight\", \"Seat height\", \"Overall height\", \"Overall length\", \"Fuel capacity\", \"Oil capacity\"\n",
    "]\n",
    "\n",
    "PYPPETEER_CHROMIUM_REVISION = '1263111'\n",
    "\n",
    "os.environ['PYPPETEER_CHROMIUM_REVISION'] = PYPPETEER_CHROMIUM_REVISION\n",
    "\n",
    "import logging\n",
    "\n",
    "# Deactivate websockets and urllib3 logs:\n",
    "logging.getLogger(\"websockets\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e59c9f2-4fae-4450-8494-068b86bac3e2",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to load scraped URLS from file:\n",
    "def load_scraped_urls(filename):\n",
    "    \"\"\"Load already scraped URLs from a text file.\"\"\"\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, \"r\") as f:\n",
    "            return set(line.strip() for line in f)\n",
    "    return set()\n",
    "\n",
    "# Function to save URL in text file:\n",
    "def append_to_file(filename, url):\n",
    "    \"\"\"Save a new URL in the text file without overwriting existing content.\"\"\"\n",
    "    with open(filename, \"a\") as f:\n",
    "        f.write(url + \"\\n\")\n",
    "\n",
    "# Function to save final DataFrame:\n",
    "def save_final_data(data_list, csv_file, scraped_urls):\n",
    "    if not data_list:\n",
    "        print(\"No new data to save.\")\n",
    "        return\n",
    "\n",
    "    # Load file if it already exists:\n",
    "    if os.path.exists(csv_file):\n",
    "        df_existing = pd.read_csv(csv_file)\n",
    "    else:\n",
    "        df_existing = pd.DataFrame()\n",
    "\n",
    "    # Create DataFrame from list:\n",
    "    df_new = pd.DataFrame(data_list)\n",
    "\n",
    "    # Concatenate and delete duplicates:\n",
    "    df_final = pd.concat([df_existing, df_new], ignore_index=True).drop_duplicates()\n",
    "\n",
    "    # Order Dataframe columns:\n",
    "    df_final = df_final[column_order]\n",
    "\n",
    "    # Save CSV file:\n",
    "    df_final.to_csv(csv_file, index=False)\n",
    "    print(f\"New {len(df_new)} motorbikes have been saved in {csv_file}\")\n",
    "\n",
    "    # Update URLs text file:\n",
    "    for url in scraped_urls:  # For each URL\n",
    "        append_to_file(scraped_FILE, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa10393-b4dc-4d6f-9823-acd4749f109c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to obtain the list of available years in Bikez.com\n",
    "def obtain_years():\n",
    "    url = \"https://bikez.com/years/index.php\"\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "    body = soup.find(\"table\", class_=\"zebra\")\n",
    "    year_links = body.find_all(\"a\", href=re.compile(r\"-motorcycle-models.php\"))\n",
    "\n",
    "    years = []\n",
    "    for link in year_links:\n",
    "        text = link.text.strip()\n",
    "        year = text.split()[0]\n",
    "        years.append(year)\n",
    "\n",
    "    return sorted(years)\n",
    "\n",
    "\"\"\"\n",
    "# TEST:\n",
    "year_list = obtain_years()  \n",
    "years_URLs = []\n",
    "for year in year_list:\n",
    "    years_URLs.append(\"https://bikez.com/year/\" + year + \"-motorcycle-models.php\")\n",
    "print(years_URLs)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65e1f1e-9a89-4571-9979-b8fedc692e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to obtain list of motorbikes from a certain year in Bikez.com\n",
    "def obtain_models(year):\n",
    "    url = \"https://bikez.com/year/\" + str(year) + \"-motorcycle-models.php\"\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "    body = soup.find(\"table\", class_=\"zebra\")\n",
    "    model_links = body.find_all(\"a\", href=re.compile(str(year) + \".php\"))\n",
    "    \n",
    "    models = set()\n",
    "    scraped_models = load_scraped_urls(scraped_FILE)\n",
    "    \n",
    "    for link in model_links:\n",
    "        href = link.get(\"href\")\n",
    "        # Extract model name after \"/motorcycles/\" and before \".php\"\n",
    "        model = href.split(\"/\")[-1].split(\".php\")[0]\n",
    "\n",
    "        if model not in scraped_models:\n",
    "            models.add(model)\n",
    "\n",
    "    return sorted(list(models))\n",
    "\n",
    "\"\"\"\n",
    "# TEST:\n",
    "models_2025 = obtain_models(2025)\n",
    "print(models_2025)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6096da80-027f-44b0-869d-bf2ff6f10e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to obtain the information from a certain motorbike model in Bikez.com\n",
    "\n",
    "async def obtain_model_info_asyncio(url):  \n",
    "    asession = AsyncHTMLSession()\n",
    "    \n",
    "    try:\n",
    "        r = await asession.get(url, timeout = 30)\n",
    "        \n",
    "        # Render JavaScript so that dynamic content is loaded.\n",
    "        await r.html.arender(timeout=30)\n",
    "          \n",
    "        # Parse the rendered HTML content using BeautifulSoup:\n",
    "        soup = BeautifulSoup(r.html.html, 'html.parser')\n",
    "        \n",
    "        # Remove hidden elements in usefull information:\n",
    "        for hidden in soup.find_all(style=lambda value: value and \"display:none\" in value):\n",
    "            hidden.extract()\n",
    "            \n",
    "        # Find all tables with class \"Grid\":\n",
    "        tables = soup.find_all(\"table\", class_=\"Grid\")\n",
    "    \n",
    "        # Select general information table:\n",
    "        selected_table = None\n",
    "        for table in tables:\n",
    "            if any(text in table.get_text() for text in [\"General information\", \"General moped information\"]):\n",
    "                selected_table = table\n",
    "                break\n",
    "    \n",
    "        if selected_table:\n",
    "            # Dictionary\n",
    "            data  = {}\n",
    "        \n",
    "            # Data we want to get from Bikez.com:\n",
    "            table_fields = {\n",
    "                # General information:\n",
    "                \"Motorcycle name\", \"Model year\", \"Category\", \"Price as new\",\n",
    "                # Engine and transmission:\n",
    "                \"Engine size\", \"Type of engine\", \"Power output\", \"Torque\", \"Transmission type\", \"Clutch\", \"Fuel consumption\",\n",
    "                # Chassis, suspension, brakes and wheels:\n",
    "                \"Front tire\", \"Rear tire\", \"Front brakes\", \"Rear brakes\",\n",
    "                # Physical measures and capacities:\n",
    "                \"Weight incl. oil, gas, etc\", \"Dry weight\", \"Seat height\", \"Overall height\", \"Overall length\", \"Fuel capacity\", \"Oil capacity\"\n",
    "            }\n",
    "            \n",
    "            # Name mapping (found different names for same parameters through the website):\n",
    "            field_mapping = {\n",
    "                \"Engine size\": [\"Displacement\"],\n",
    "                \"Motorcycle name\": [\"Model\", \"Model name\"],\n",
    "                \"Model year\": [\"Year model\", \"Year of manufacture\", \"Year\"],\n",
    "                \"Category\": [\"Type\"],\n",
    "                \"Type of engine\": [\"Engine type\"],\n",
    "                \"Power output\": [\"Effect\", \"Output\", \"Power\"],\n",
    "                \"Price as new\": [\"Price\"]\n",
    "            }\n",
    "            \n",
    "            # Obtain rows:\n",
    "            rows = selected_table.find_all(\"tr\")\n",
    "            \n",
    "            # Process rows:\n",
    "            for row in rows:\n",
    "                columns = row.find_all(\"td\")\n",
    "                if len(columns) >= 2:  # Make sure there is at least two columns in the table (information table)\n",
    "                    clave = columns[0].get_text(strip=True)\n",
    "                    value = columns[1].get_text(strip=True)\n",
    "                    \n",
    "                    # Map field name:\n",
    "                    for standard_field, alternative_names in field_mapping.items():\n",
    "                        if clave in alternative_names:\n",
    "                            data[standard_field] = value\n",
    "                            break\n",
    "                    else:\n",
    "                        data[clave] = value\n",
    "            \n",
    "            # Dictionary with information:\n",
    "            clean_data = {key: data.get(key, \"\") for key in table_fields}\n",
    "        \n",
    "            # Show info about scraping process in console:\n",
    "            print(f\"Year\", clean_data[\"Model year\"], \"motorbike:\", clean_data[\"Motorcycle name\"], \"scraped.\")\n",
    "\n",
    "            return clean_data\n",
    "        \n",
    "        else:\n",
    "            print(\"Could not find 'General information' table.\")\n",
    "            return {}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error rendering {url}: {e}\")\n",
    "        return {}\n",
    "    \n",
    "    finally:\n",
    "        # Close scraping to free up resources (I was having issues with script crashing after a while because Chromium instances were not closing properly)\n",
    "        await asession.close()\n",
    "\n",
    "\"\"\"\n",
    "# TEST:\n",
    "url = 'https://bikez.com/motorcycles/aprilia_tuono_v4_2024.php'\n",
    "loop = asyncio.get_event_loop()\n",
    "model_info = await obtain_model_info_asyncio(url)\n",
    "print(model_info)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ee5d6c-4783-43d2-bff9-faa5b8647217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN Bikez.com:\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "data_list = []  # List to save data temporarily\n",
    "scraped_urls = load_scraped_urls(scraped_FILE)\n",
    "scraped_urls_new  = []\n",
    "\n",
    "years = obtain_years()\n",
    "\n",
    "try:\n",
    "    for year in years:\n",
    "        models = obtain_models(year)\n",
    "      \n",
    "        for model in models:\n",
    "            url = \"https://bikez.com/motorcycles/\" + model + \".php\"\n",
    "            if url in scraped_urls:    \n",
    "                print(f\"Motorbike {model} has already been scraped.\")\n",
    "                continue\n",
    "                \n",
    "            # Get model information:\n",
    "            data_model = await obtain_model_info_asyncio(url)\n",
    "            if data_model:\n",
    "                data_list.append(data_model)\n",
    "                scraped_urls_new.append(url) \n",
    "\n",
    "            # Deal with any unexpected crash saving data every 50 iterations:\n",
    "            if not len(data_list) % 50:\n",
    "                save_final_data(data_list, CSV_FILE, scraped_urls_new)\n",
    "                print(\"Safety save every 50 iterations.\")\n",
    "                # Reset variables:\n",
    "                data_list = []\n",
    "                scraped_urls_new = []\n",
    "\n",
    "    # Save CSV file at the end of scraping:\n",
    "    save_final_data(data_list, CSV_FILE, scraped_urls_new)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Critical error: {e}\")\n",
    "    # Save CSV file in case of error happening:\n",
    "    save_final_data(data_list, CSV_FILE, scraped_urls_new)\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Total execution time of: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a615ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
